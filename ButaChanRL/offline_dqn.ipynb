{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964e9835-19e1-4b04-ad98-6a289baea53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c384a4de-20f5-4377-aae7-79b4cb321f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, n_states,n_actions,n_hidden_units):\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_dim = n_states\n",
    "        self.num_hidden_units = n_hidden_units\n",
    "        \n",
    "        self.num_actions = n_actions\n",
    "        self.layer1 = torch.nn.Linear(self.state_dim, self.num_hidden_units)\n",
    "        self.layer2 = torch.nn.Linear(self.num_hidden_units,self.num_hidden_units)\n",
    "        self.layer3 = torch.nn.Linear(self.num_hidden_units, self.num_actions)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.layer_norm = torch.nn.LayerNorm(self.num_hidden_units)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f86e74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, minibatch_size, observation_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        #random.seed(seed)\n",
    "        self.max_size = buffer_size\n",
    "        self.pos = 0\n",
    "        self.full = False\n",
    "        self.states = np.zeros((self.max_size,observation_size))\n",
    "        self.next_states = np.zeros((self.max_size,observation_size))\n",
    "        self.actions = np.zeros(self.max_size,dtype=np.int8)\n",
    "        self.rewards = np.zeros(self.max_size)\n",
    "        self.terminals = np.zeros(self.max_size,dtype=np.int8)\n",
    "\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        self.states[self.pos] = state\n",
    "        self.actions[self.pos] = action\n",
    "        self.rewards[self.pos] = reward\n",
    "        self.terminals[self.pos] = terminal\n",
    "        self.next_states[self.pos] = next_state\n",
    "        self.pos += 1\n",
    "        if(self.pos==self.max_size):\n",
    "            self.pos = 0\n",
    "            self.full = True\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        if(self.full):\n",
    "            idxs = np.random.randint(0,self.max_size,size=self.minibatch_size) \n",
    "        else:\n",
    "            idxs = np.random.randint(0,self.pos,size=self.minibatch_size)\n",
    "        sample_ = [self.states[idxs],self.actions[idxs],self.rewards[idxs],self.terminals[idxs],\n",
    "                   self.next_states[idxs]]\n",
    "        #print(sample_)\n",
    "        return sample_\n",
    "\n",
    "    def size(self):\n",
    "        if(self.full):\n",
    "            return self.max_size\n",
    "        else:\n",
    "            return self.pos\n",
    "    \n",
    "    def reset(self):\n",
    "        self.full = False\n",
    "        self.pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c3ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_error_double_dqn(states, next_states, actions, rewards, discount, terminals, target_network, current_q_network):\n",
    "    with torch.no_grad():\n",
    "        # The idea of Double DQN is to get max actions from current network\n",
    "        # and to get Q values from target_network for next states. \n",
    "        q_next_mat = current_q_network(next_states)\n",
    "        max_actions = torch.argmax(q_next_mat,1)\n",
    "        double_q_mat = target_network(next_states)\n",
    "    batch_indices = torch.arange(q_next_mat.shape[0])\n",
    "    double_q_max = double_q_mat[batch_indices,max_actions]\n",
    "    target_vec = rewards+discount*double_q_max*(torch.ones_like(terminals)-terminals)\n",
    "    q_mat = current_q_network(states)\n",
    "    batch_indices = torch.arange(q_mat.shape[0])\n",
    "    q_vec = q_mat[batch_indices,actions]\n",
    "    #delta_vec = target_vec - q_vec\n",
    "    return target_vec,q_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3ef891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, target_network, current_q_network,device,double_dqn=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions,\n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets,\n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states = experiences[0]\n",
    "    actions = experiences[1]\n",
    "    rewards = experiences[2]\n",
    "    terminals = experiences[3]\n",
    "    next_states = experiences[4]\n",
    "    # numpy arrays to tensors\n",
    "    states = torch.tensor(states,dtype=torch.float32,device=device)\n",
    "    next_states = torch.tensor(next_states,dtype=torch.float32,device=device)\n",
    "    rewards = torch.tensor(rewards,dtype=torch.float32,device=device)\n",
    "    terminals = torch.tensor(terminals,dtype=torch.int,device=device)\n",
    "    actions = torch.tensor(actions,dtype=torch.int,device=device)\n",
    " \n",
    "    # Compute TD error using the get_td_error function\n",
    "    # Note that q_vec is a 1D array of shape (batch_size)\n",
    "    target_vec,q_vec = get_td_error_double_dqn(states, next_states, actions, rewards, discount, terminals, target_network, current_q_network)\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "    loss = loss_fun(target_vec,q_vec)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(target_network.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    return loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.name = \"DQN\"\n",
    "        self.device = None\n",
    "        self.seed = 1 # random seed. Later can be changed by using set_seed method\n",
    "\n",
    "    def set_seed(self,seed=1):\n",
    "        self.seed = seed\n",
    "        #random.seed(self.seed)\n",
    "    \n",
    "    def set_epsilon_decay(self,n_steps=10000):\n",
    "        self.eps_decay = 1. - 1./n_steps\n",
    "\n",
    "    def set_device(self,device):\n",
    "        self.device = device\n",
    "    \n",
    "    def agent_init(self, agent_config):\n",
    "        if(self.device==None):\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'],\n",
    "                                          agent_config['minibatch_sz'],\n",
    "                                          agent_config['observation_size'])\n",
    "        self.state_dim = agent_config[\"network_config\"].get(\"state_dim\")\n",
    "        self.num_hidden_layers = agent_config[\"network_config\"].get(\"num_hidden_units\")\n",
    "        self.num_actions = agent_config[\"network_config\"].get(\"num_actions\")\n",
    "        \n",
    "        self.network_type = agent_config[\"network_config\"].get(\"network_type\")\n",
    "        \n",
    "        self.q_network = DQN().to(self.device)\n",
    "        self.target_network = DQN(agent_config['network_config']).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.step_size = agent_config['step_size']\n",
    "        self.double_dqn = agent_config['double_dqn']\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.epsilon = agent_config['epsilon']\n",
    "        self.time_step = 0\n",
    "        self.update_freq = agent_config['update_freq']\n",
    "        self.loss = []\n",
    "        self.episode_rewards = []\n",
    "        self.loss_capacity = 5_000\n",
    "        self.warmup_steps = agent_config['warmup_steps']\n",
    "        self.eps_decay = 0.9999\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(),lr=self.step_size,weight_decay=0.01)\n",
    "\n",
    "    def greedy_policy(self,state,epsilon=0.001):\n",
    "        state = torch.tensor([state],dtype=torch.float32,device=self.device)\n",
    "        a = random.random()\n",
    "        if(a>=epsilon):\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "            action = torch.argmax(action_values).item()\n",
    "        else:\n",
    "            action = random.choice(list(range(self.num_actions)))\n",
    "        return action\n",
    "\n",
    "    def epsilon_greedy_policy(self,state):\n",
    "        epsilon = np.max([self.epsilon,0.05]) \n",
    "        state = torch.tensor([state],dtype=torch.float32,requires_grad=False,device=self.device)\n",
    "        self.epsilon *= self.eps_decay\n",
    "        a = random.random()\n",
    "        if(a>=epsilon):\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "            action = torch.argmax(action_values).item()\n",
    "        else:\n",
    "            action = random.choice(list(range(self.num_actions)))\n",
    "        return action\n",
    "\n",
    "    # Work Required: No.\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = state #torch.tensor(np.array([state]),dtype=torch.float32,device=self.device)\n",
    "        self.last_action = self.epsilon_greedy_policy(self.last_state)\n",
    "        self.time_step += 1\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        #state = torch.tensor(np.array([state]),dtype=torch.float32,device=self.device)\n",
    "        action = self.epsilon_greedy_policy(state)\n",
    "        terminal = False\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state)\n",
    "\n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size and self.time_step>self.warmup_steps: # and self.episode_steps%self.replay_buffer.minibatch_size==0:\n",
    "            for _ in range(self.num_replay):\n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                loss = optimize_network(experiences, self.discount, self.optimizer, self.target_network, self.q_network,self.device,self.double_dqn)\n",
    "                if(len(self.loss)>=self.loss_capacity):\n",
    "                    del self.loss[0]\n",
    "                self.loss.append(loss)\n",
    "\n",
    "        if(self.time_step%self.update_freq==0):\n",
    "            #print(\"Updating network\")\n",
    "            self.update_target_network()\n",
    "       \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "        ### END CODE HERE\n",
    "        # your code here\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        self.time_step += 1\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        self.episode_rewards.append(self.sum_rewards)\n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state) #torch.zeros_like(self.last_state,device=self.device)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "        end_loss = 0\n",
    "        # your code here\n",
    "        terminal = True\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state)\n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                loss = optimize_network(experiences, self.discount, self.optimizer, self.target_network, self.q_network,self.device,self.double_dqn)\n",
    "                end_loss = loss\n",
    "                if(len(self.loss)>=self.loss_capacity):\n",
    "                    del self.loss[0]\n",
    "                self.loss.append(loss)\n",
    "        \n",
    "        if(self.time_step%self.update_freq==0):\n",
    "            self.update_target_network()\n",
    "        self.time_step += 1\n",
    "    \n",
    "        return end_loss\n",
    "\n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def get_loss(self):\n",
    "        return np.average(np.array(self.loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

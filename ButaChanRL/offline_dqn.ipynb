{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "964e9835-19e1-4b04-ad98-6a289baea53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "c384a4de-20f5-4377-aae7-79b4cb321f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, n_states,n_actions,n_hidden_units):\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_dim = n_states\n",
    "        self.num_hidden_units = n_hidden_units\n",
    "        \n",
    "        self.num_actions = n_actions\n",
    "        self.layer1 = torch.nn.Linear(self.state_dim, self.num_hidden_units)\n",
    "        self.layer2 = torch.nn.Linear(self.num_hidden_units,self.num_hidden_units)\n",
    "        self.layer3 = torch.nn.Linear(self.num_hidden_units, self.num_actions)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.layer_norm = torch.nn.LayerNorm(self.num_hidden_units)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.layer_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.layer_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "91d42971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_to_JSON(replay_buffer,buffer_size):\n",
    "    # Experience Replay stores transitions as numpy arrays for performance reasons\n",
    "    # Need to transform this to a list in order to write properly to JSON\n",
    "    \"\"\" Transforms replay buffer data to JSON data type\n",
    "    \"\"\"\n",
    "    j = []\n",
    "    for i in range(buffer_size):\n",
    "        transition = {\n",
    "            \"state\":replay_buffer.states[i].tolist(),\n",
    "            \"action\":int(replay_buffer.actions[i]),\n",
    "            \"reward\":float(replay_buffer.rewards[i]),\n",
    "            \"terminal\":int(replay_buffer.terminals[i]),\n",
    "            \"next_state\":replay_buffer.next_states[i].tolist()\n",
    "        }\n",
    "        j.append(transition)\n",
    "    return j\n",
    "\n",
    "def json_to_buffer(json_data,replay_buffer):\n",
    "    \"\"\" Transfers JSON data to replay buffer directly\n",
    "    \"\"\"\n",
    "    assert len(json_data)<=replay_buffer.max_size\n",
    "    for i,j in enumerate(json_data):\n",
    "        state = j[\"state\"]\n",
    "        action = j[\"action\"]\n",
    "        reward = j[\"reward\"]\n",
    "        terminal = j[\"terminal\"]\n",
    "        next_state=j[\"next_state\"]\n",
    "        replay_buffer.states[i] = state[0]\n",
    "        replay_buffer.actions[i] = action\n",
    "        replay_buffer.rewards[i] = reward\n",
    "        replay_buffer.terminals[i] = terminal\n",
    "        replay_buffer.next_states[i] = next_state[0]\n",
    "    return len(json_data)\n",
    "    \n",
    "def save_json(path, data):\n",
    "    \"\"\"save json data\n",
    "    Args:\n",
    "        path (Path): path to json file\n",
    "        data (dict): data to be saved in json file\n",
    "    \"\"\"\n",
    "    with open(path,\"w\") as f:\n",
    "        json.dump(data,f,indent=2)\n",
    "\n",
    "def read_json(path):\n",
    "    \"\"\"read json file for stored ReplayBuffer data\n",
    "    Args:\n",
    "        path (path): path to json file\n",
    "    Returns:\n",
    "        data (list): data to be used for Expereince Replay\n",
    "    \"\"\"\n",
    "    f = open(path)\n",
    "    json_data = json.load(f)\n",
    "    return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "f86e74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, minibatch_size, observation_size):\n",
    "        #self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        #random.seed(seed)\n",
    "        self.max_size = buffer_size\n",
    "        self.pos = 0\n",
    "        self.full = False\n",
    "        self.states = np.zeros((self.max_size,observation_size))\n",
    "        self.next_states = np.zeros((self.max_size,observation_size))\n",
    "        self.actions = np.zeros(self.max_size,dtype=np.int8)\n",
    "        self.rewards = np.zeros(self.max_size)\n",
    "        self.terminals = np.zeros(self.max_size,dtype=np.int8)\n",
    "        self.rand_generator = np.random.RandomState()\n",
    "\n",
    "    def set_seed(self,seed=1):\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        self.states[self.pos] = state\n",
    "        self.actions[self.pos] = action\n",
    "        self.rewards[self.pos] = reward\n",
    "        self.terminals[self.pos] = terminal\n",
    "        self.next_states[self.pos] = next_state\n",
    "        self.pos += 1\n",
    "        if(self.pos==self.max_size):\n",
    "            self.pos = 0\n",
    "            self.full = True\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        if(self.full):\n",
    "            idxs = self.rand_generator.randint(0,self.max_size,size=self.minibatch_size) \n",
    "        else:\n",
    "            idxs = self.rand_generator.randint(0,self.pos,size=self.minibatch_size)\n",
    "        sample_ = [self.states[idxs],self.actions[idxs],self.rewards[idxs],self.terminals[idxs],\n",
    "                   self.next_states[idxs]]\n",
    "        #print(sample_)\n",
    "        return sample_\n",
    "\n",
    "    def size(self):\n",
    "        if(self.full):\n",
    "            return self.max_size\n",
    "        else:\n",
    "            return self.pos\n",
    "        \n",
    "    def save_buffer(self,path=\"ReplayBuffer.JSON\"):\n",
    "        json_data = buffer_to_JSON(self,self.size())\n",
    "        save_json(path,json_data)\n",
    "\n",
    "    def load_buffer(self,path=\"ReplayBuffer.JSON\"):\n",
    "        self.reset()\n",
    "        json_data = read_json(path)\n",
    "        data_size=json_to_buffer(json_data,self)\n",
    "        if(data_size==self.max_size):\n",
    "            self.full = True\n",
    "            self.pos = 0\n",
    "        else:\n",
    "            self.pos = data_size\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.full = False\n",
    "        self.pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "f9c3ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_error(states, next_states, actions, rewards, discount, terminals, target_network, current_q_network):\n",
    "    with torch.no_grad():\n",
    "        # The idea of Double DQN is to get max actions from current network\n",
    "        # and to get Q values from target_network for next states. \n",
    "        q_next_mat = current_q_network(next_states)\n",
    "        max_actions = torch.argmax(q_next_mat,1)\n",
    "        double_q_mat = target_network(next_states)\n",
    "    batch_indices = torch.arange(q_next_mat.shape[0])\n",
    "    double_q_max = double_q_mat[batch_indices,max_actions]\n",
    "    target_vec = rewards+discount*double_q_max*(torch.ones_like(terminals)-terminals)\n",
    "    q_mat = current_q_network(states)\n",
    "    batch_indices = torch.arange(q_mat.shape[0])\n",
    "    q_vec = q_mat[batch_indices,actions]\n",
    "    #delta_vec = target_vec - q_vec\n",
    "    return target_vec,q_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "bd3ef891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, target_network, current_q_network,device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions,\n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets,\n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states = experiences[0]\n",
    "    actions = experiences[1]\n",
    "    rewards = experiences[2]\n",
    "    terminals = experiences[3]\n",
    "    next_states = experiences[4]\n",
    "    # numpy arrays to tensors and move to device (cpu or gpu)\n",
    "    states = torch.tensor(states,dtype=torch.float32,device=device)\n",
    "    next_states = torch.tensor(next_states,dtype=torch.float32,device=device)\n",
    "    rewards = torch.tensor(rewards,dtype=torch.float32,device=device)\n",
    "    terminals = torch.tensor(terminals,dtype=torch.int,device=device)\n",
    "    actions = torch.tensor(actions,dtype=torch.int,device=device)\n",
    " \n",
    "    # Compute TD error using the get_td_error function\n",
    "    # Note that q_vec is a 1D array of shape (batch_size)\n",
    "    target_vec,q_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, target_network, current_q_network)\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "    loss = loss_fun(target_vec,q_vec)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(current_q_network.parameters(), 10)\n",
    "    optimizer.step()\n",
    "    return loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "d969fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,buffer_config):\n",
    "        self.name = \"DQN\"\n",
    "        self.device = None\n",
    "        self.rand_generator = np.random.RandomState() # random seed. Later can be changed by using set_seed method\n",
    "        self.replay_buffer = ReplayBuffer(buffer_config[\"replay_buffer_size\"],\n",
    "                                          buffer_config[\"minibatch_sz\"],\n",
    "                                          buffer_config[\"observation_size\"])\n",
    "\n",
    "    def set_seed(self,seed=1):\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        #random.seed(self.seed)\n",
    "    \n",
    "    def set_epsilon_decay(self,n_steps=10000):\n",
    "        self.eps_decay = 1. - 1./n_steps\n",
    "\n",
    "    def set_device(self,device=\"cpu\"):\n",
    "        if(device==\"cuda\"):\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "    \n",
    "    def agent_init(self, agent_config):\n",
    "        if(self.device==None):\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.state_dim = agent_config[\"network_config\"].get(\"state_dim\")\n",
    "        self.num_hidden_layers = agent_config[\"network_config\"].get(\"num_hidden_units\")\n",
    "        self.num_actions = agent_config[\"network_config\"].get(\"num_actions\")\n",
    "        \n",
    "        self.network_type = agent_config[\"network_config\"].get(\"network_type\")\n",
    "        \n",
    "        self.q_network = DQN(self.state_dim,self.num_actions,self.num_hidden_layers).to(self.device)\n",
    "        self.target_network = DQN(self.state_dim,self.num_actions,self.num_hidden_layers).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.step_size = agent_config['step_size']\n",
    "        self.double_dqn = agent_config['double_dqn']\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.epsilon = agent_config['epsilon']\n",
    "        self.time_step = 0\n",
    "        self.update_freq = agent_config['update_freq']\n",
    "        self.loss = []\n",
    "        self.episode_rewards = []\n",
    "        self.loss_capacity = 5_000\n",
    "        self.warmup_steps = agent_config['warmup_steps']\n",
    "        self.eps_decay = 0.9999\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(),lr=self.step_size,weight_decay=0.01)\n",
    "        self.soft_update = True\n",
    "        self.tau = 0.005\n",
    "\n",
    "    def greedy_policy(self,state,epsilon=0.001):\n",
    "        state = torch.tensor(state,dtype=torch.float32,device=self.device)\n",
    "        a = self.rand_generator.rand()\n",
    "        if(a>=epsilon):\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "            action = torch.argmax(action_values).item()\n",
    "        else:\n",
    "            action = self.rand_generator.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    def epsilon_greedy_policy(self,state):\n",
    "        epsilon = np.max([self.epsilon,0.05]) \n",
    "        state = torch.tensor(state,dtype=torch.float32,requires_grad=False,device=self.device)\n",
    "        self.epsilon *= self.eps_decay\n",
    "        a = self.rand_generator.rand()\n",
    "        if(a>=epsilon):\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "            action = torch.argmax(action_values).item()\n",
    "        else:\n",
    "            action = self.rand_generator.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    # Work Required: No.\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = state #torch.tensor(np.array([state]),dtype=torch.float32,device=self.device)\n",
    "        self.last_action = self.epsilon_greedy_policy(self.last_state)\n",
    "        self.time_step += 1\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        #state = torch.tensor(np.array([state]),dtype=torch.float32,device=self.device)\n",
    "        action = self.epsilon_greedy_policy(state)\n",
    "        terminal = False\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state)\n",
    "\n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size and self.time_step>self.warmup_steps: # and self.episode_steps%self.replay_buffer.minibatch_size==0:\n",
    "            for _ in range(self.num_replay):\n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                loss = optimize_network(experiences, self.discount, self.optimizer, self.target_network, self.q_network,self.device)\n",
    "                if(len(self.loss)>=self.loss_capacity):\n",
    "                    del self.loss[0]\n",
    "                self.loss.append(loss)\n",
    "\n",
    "        if(self.soft_update):\n",
    "            self.polyak_update_target_network()\n",
    "        else:\n",
    "            if(self.time_step%self.update_freq==0):\n",
    "                #print(\"Updating network\")\n",
    "                self.update_target_network()\n",
    "       \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "        ### END CODE HERE\n",
    "        # your code here\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        self.time_step += 1\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        self.episode_rewards.append(self.sum_rewards)\n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state) #torch.zeros_like(self.last_state,device=self.device)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "        end_loss = 0\n",
    "        # your code here\n",
    "        terminal = True\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state)\n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                loss = optimize_network(experiences, self.discount, self.optimizer, self.target_network, self.q_network,self.device)\n",
    "                end_loss = loss\n",
    "                if(len(self.loss)>=self.loss_capacity):\n",
    "                    del self.loss[0]\n",
    "                self.loss.append(loss)\n",
    "        \n",
    "        if(self.soft_update):\n",
    "            self.polyak_update_target_network()\n",
    "        else:\n",
    "            if(self.time_step%self.update_freq==0):\n",
    "                #print(\"Updating network\")\n",
    "                self.update_target_network()\n",
    "                \n",
    "        self.time_step += 1\n",
    "    \n",
    "        return end_loss\n",
    "\n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def polyak_update_target_network(self):\n",
    "        one = torch.ones(1, requires_grad=False).to(self.device)\n",
    "        for param, target_param in zip(self.q_network.parameters(), self.target_network.parameters()):\n",
    "            target_param.data.mul_(1-self.tau)\n",
    "            target_param.data.addcmul_(param.data, one, value=self.tau)\n",
    "\n",
    "    def get_loss(self):\n",
    "        return np.average(np.array(self.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "0533d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class offlineDQNAgent:\n",
    "    def __init__(self,buffer_config):\n",
    "        self.name = \"offlineDQN\"\n",
    "        self.device = None\n",
    "        self.rand_generator = np.random.RandomState() # random seed. Later can be changed by using set_seed method\n",
    "        self.replay_buffer = ReplayBuffer(buffer_config[\"replay_buffer_size\"],\n",
    "                                          buffer_config[\"minibatch_sz\"],\n",
    "                                          buffer_config[\"observation_size\"])\n",
    "        self.buffer_path = \"experienceReplay.json\"\n",
    "\n",
    "    def set_seed(self,seed=1):\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        #random.seed(self.seed)\n",
    "    \n",
    "    def set_epsilon_decay(self,n_steps=10000):\n",
    "        self.eps_decay = 1. - 1./n_steps\n",
    "\n",
    "    def set_buffer_path(self,path=\"experienceReplay.json\"):\n",
    "        self.buffer_path = path\n",
    "    \n",
    "\n",
    "    def set_device(self,device=\"cpu\"):\n",
    "        if(device==\"cuda\"):\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "    \n",
    "    def agent_init(self, agent_config):\n",
    "        if(self.device==None):\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.state_dim = agent_config[\"network_config\"].get(\"state_dim\")\n",
    "        self.num_hidden_layers = agent_config[\"network_config\"].get(\"num_hidden_units\")\n",
    "        self.num_actions = agent_config[\"network_config\"].get(\"num_actions\")\n",
    "        self.network_type = agent_config[\"network_config\"].get(\"network_type\")\n",
    "        \n",
    "        self.q_network = DQN(self.state_dim,self.num_actions,self.num_hidden_layers).to(self.device)\n",
    "        self.target_network = DQN(self.state_dim,self.num_actions,self.num_hidden_layers).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.step_size = agent_config['step_size']\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        \n",
    "        self.time_step = 0\n",
    "        \n",
    "        self.loss = []\n",
    "        self.episode_rewards = []\n",
    "        self.loss_capacity = 5_000\n",
    "        self.epsilon = agent_config['epsilon']\n",
    "        self.time_step = 0\n",
    "        self.update_freq = agent_config['update_freq']\n",
    "    \n",
    "        self.eps_decay = 0.9999\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(),lr=self.step_size,weight_decay=0.01)\n",
    "        self.tau = 0.005\n",
    "        self.replay_buffer.load_buffer(self.buffer_path)\n",
    "\n",
    "    def epsilon_greedy_policy(self,state):\n",
    "        epsilon = np.max([self.epsilon,0.05]) \n",
    "        state = torch.tensor(state,dtype=torch.float32,requires_grad=False,device=self.device)\n",
    "        self.epsilon *= self.eps_decay\n",
    "        a = self.rand_generator.rand()\n",
    "        if(a>=epsilon):\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "            action = torch.argmax(action_values).item()\n",
    "        else:\n",
    "            action = self.rand_generator.choice(self.num_actions)\n",
    "        return action\n",
    "\n",
    "    def greedy_policy(self,state,epsilon=0.001):\n",
    "        state = torch.tensor(state,dtype=torch.float32,device=self.device)\n",
    "        a = self.rand_generator.rand()\n",
    "        if(a>=epsilon):\n",
    "            with torch.no_grad():\n",
    "                action_values = self.q_network(state)\n",
    "            action = torch.argmax(action_values).item()\n",
    "        else:\n",
    "            action = self.rand_generator.choice(self.num_actions)\n",
    "        return action\n",
    "    \n",
    "    def load_buffer(self):\n",
    "        self.replay_buffer.load_buffer()\n",
    "\n",
    "    def learn_offline(self):\n",
    "        self.episode_steps += 1\n",
    "        # Perform replay steps:\n",
    "        #if self.replay_buffer.size() > self.replay_buffer.minibatch_size and self.time_step>self.warmup_steps: # and self.episode_steps%self.replay_buffer.minibatch_size==0:\n",
    "        for _ in range(self.num_replay):\n",
    "            # Get sample experiences from the replay buffer\n",
    "            experiences = self.replay_buffer.sample()\n",
    "            loss = optimize_network(experiences, self.discount, self.optimizer, self.target_network, self.q_network,self.device)\n",
    "            if(len(self.loss)>=self.loss_capacity):\n",
    "                del self.loss[0]\n",
    "            self.loss.append(loss)\n",
    "        self.update_target_network()\n",
    "\n",
    "    # Work Required: No.\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = state #torch.tensor(np.array([state]),dtype=torch.float32,device=self.device)\n",
    "        self.last_action = self.epsilon_greedy_policy(self.last_state)\n",
    "        self.time_step += 1\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        #state = torch.tensor(np.array([state]),dtype=torch.float32,device=self.device)\n",
    "        action = self.epsilon_greedy_policy(state)\n",
    "        terminal = False\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state)\n",
    "\n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size: # and self.episode_steps%self.replay_buffer.minibatch_size==0:\n",
    "            for _ in range(self.num_replay):\n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                loss = optimize_network(experiences, self.discount, self.optimizer, self.target_network, self.q_network,self.device)\n",
    "                if(len(self.loss)>=self.loss_capacity):\n",
    "                    del self.loss[0]\n",
    "                self.loss.append(loss)\n",
    "            self.update_target_network()\n",
    "       \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "        ### END CODE HERE\n",
    "        # your code here\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        self.time_step += 1\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        self.episode_rewards.append(self.sum_rewards)\n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state) #torch.zeros_like(self.last_state,device=self.device)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "        end_loss = 0\n",
    "        # your code here\n",
    "        terminal = True\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state)\n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                loss = optimize_network(experiences, self.discount, self.optimizer, self.target_network, self.q_network,self.device)\n",
    "                end_loss = loss\n",
    "                if(len(self.loss)>=self.loss_capacity):\n",
    "                    del self.loss[0]\n",
    "                self.loss.append(loss)\n",
    "            self.update_target_network()\n",
    "                \n",
    "        self.time_step += 1\n",
    "    \n",
    "        return end_loss\n",
    "\n",
    "    def update_target_network(self):\n",
    "        one = torch.ones(1, requires_grad=False).to(self.device)\n",
    "        for param, target_param in zip(self.q_network.parameters(), self.target_network.parameters()):\n",
    "            target_param.data.mul_(1-self.tau)\n",
    "            target_param.data.addcmul_(param.data, one, value=self.tau)\n",
    "\n",
    "    def get_loss(self):\n",
    "        return np.average(np.array(self.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "5a6f5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL:\n",
    "    def __init__(self) -> None:\n",
    "        self.name = \"ButaChanRL\"\n",
    "        self.mean_episode_length = 0\n",
    "        self.mean_episode_rew = 0\n",
    "        self.mean_loss= 0\n",
    "        self.step = 0\n",
    "        self.output_step = 0\n",
    "        self.epsiode_rewards = []\n",
    "        self.episode_lens = []\n",
    "        self.loss = []\n",
    "        \n",
    "        self.model_dir = \"./models/\"\n",
    "        self.num_episodes = 0\n",
    "        self.average_over = 20\n",
    "\n",
    "    def set_output_step(self,output_step):\n",
    "        self.output_step = output_step\n",
    "\n",
    "    def set_model_dir(self,name):\n",
    "        self.model_dir = name\n",
    "\n",
    "    def create_model_dir(self):\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "\n",
    "    def load_model(self,model,filename=\"model.weights\"):\n",
    "        if(model.name == \"DQN\"):\n",
    "            model.target_network.load_state_dict(torch.load(filename))\n",
    "            model.q_network.load_state_dict(torch.load(filename))\n",
    "        elif(model.name==\"ActorCritic\"):\n",
    "            model.actor_critic_network.load_state_dict(torch.load(filename))\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "\n",
    "    def save_model(self,model,filename=\"model.weights\"):\n",
    "        if(model.name == \"DQN\"):\n",
    "            torch.save(model.q_network.state_dict(),filename)\n",
    "        elif(model.name==\"ActorCritic\"):\n",
    "            torch.save(model.actor_critic_network.state_dict(),filename)\n",
    "\n",
    "     \n",
    "    def plot_live(self,data,n_mean=20,plot_start=20):\n",
    "        plt.ion()\n",
    "        plt.figure(1)\n",
    "        plot_data = torch.tensor(data, dtype=torch.float)\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode Reward')\n",
    "        plt.plot(plot_data.numpy(),\"o\")\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(plot_data ) >= plot_start:\n",
    "            means = plot_data .unfold(0, n_mean, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(n_mean), means))\n",
    "            plt.plot(means.numpy())\n",
    "        plt.pause(0.1)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "    def episode_summarize(self,episode,episode_reward):\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    def summarize(self):\n",
    "        self.mean_episode_length = 0\n",
    "        self.mean_episode_rew = 0\n",
    "        if(len(self.episode_lens)>0):\n",
    "            if(len(self.episode_lens)>self.average_over):\n",
    "                self.mean_episode_length = np.average(self.episode_lens[-self.average_over:-1])\n",
    "                self.mean_episode_rew = np.average(self.epsiode_rewards[-self.average_over:-1])\n",
    "            else:\n",
    "                self.mean_episode_length = np.average(self.episode_lens)\n",
    "                self.mean_episode_rew = np.average(self.epsiode_rewards)\n",
    "        self.mean_loss = 0\n",
    "        if(len(self.loss)>0):\n",
    "            self.mean_loss = np.average(self.loss)\n",
    "        print(f\"Step:{self.step}, Episode:{self.num_episodes} Mean_Epi_Len: {self.mean_episode_length:5.2f},Mean_Epi_Rew {self.mean_episode_rew:5.2f}, Loss: {self.mean_loss:5.2f}\")\n",
    "\n",
    "\n",
    "    def learn(self,agent,env,agent_parameters,NSTEPS=10000,visualize=False,output_step=1000,save_best_weights=False):\n",
    "        epsiode = 1\n",
    "        \n",
    "        self.output_step = output_step\n",
    "        # prepare agent\n",
    "        agent.agent_init(agent_parameters)\n",
    "        \n",
    "        #agent.set_epsilon_decay(NSTEPS//2)\n",
    "        state,info= env.reset() \n",
    "        #state = torch.tensor(state,dtype=torch.float32,device=agent.device)\n",
    "        # choose initial action based on agent's results\n",
    "        action = agent.agent_start(state)\n",
    "        done = False\n",
    "        epsiode_reward = 0\n",
    "        episode_len = 0\n",
    "        \n",
    "        for i in tqdm(range(1,NSTEPS+1)):\n",
    "            self.step = i\n",
    "            #print(action)\n",
    "            state,reward,terminated,truncated,info=env.step(action)\n",
    "            #state = torch.tensor(state,dtype=torch.float32,device=agent.device)\n",
    "            #state = torch.unsqueeze(state,0)\n",
    "            #print(i,state,reward,action,done)\n",
    "            epsiode_reward += reward\n",
    "            done = terminated or truncated\n",
    "            if(self.output_step>0 and self.step%self.output_step==0):\n",
    "                self.summarize()\n",
    "                #print(f\"Epsilon {agent.epsilon:>5.3f}\")\n",
    "                if(visualize):\n",
    "                    if(len(self.epsiode_rewards)>0):\n",
    "                        self.plot_live(self.epsiode_rewards)\n",
    "            if(done):\n",
    "                loss = agent.agent_end(reward)\n",
    "                #print(\"Loss length\",len(agent.loss))\n",
    "                self.loss.append(loss)\n",
    "                \n",
    "                if(save_best_weights):\n",
    "                    self.create_model_dir()\n",
    "                    if(len(self.epsiode_rewards)==0):\n",
    "                        model_name = self.model_dir+f\"model_{self.step}\"\n",
    "                        self.save_model(agent,model_name)\n",
    "                    else:\n",
    "                        if(epsiode_reward>max(self.epsiode_rewards)):\n",
    "                            model_name = self.model_dir+f\"model_{self.step}\"\n",
    "                            self.save_model(agent,model_name)\n",
    "                self.epsiode_rewards.append(epsiode_reward)\n",
    "                self.episode_lens.append(episode_len)\n",
    "                epsiode += 1\n",
    "                self.num_episodes += 1\n",
    "                # restart next episode\n",
    "                state,_= env.reset() \n",
    "                #state = torch.tensor(state,dtype=torch.float32,device=agent.device)\n",
    "                #state = torch.unsqueeze(state,0)\n",
    "                action = agent.agent_start(state)\n",
    "                done = False\n",
    "                epsiode_reward = 0\n",
    "                episode_len = 0\n",
    "            else:\n",
    "                action = agent.agent_step(reward,state)\n",
    "                episode_len+=1\n",
    "        return agent\n",
    "    \n",
    "    def learn_offline(self,agent,env,agent_parameters,NSTEPS=10000,output_step=1000):\n",
    "        self.output_step = output_step\n",
    "        # prepare agent\n",
    "        agent.agent_init(agent_parameters)\n",
    "        for i in tqdm(range(1,NSTEPS+1)):\n",
    "            agent.learn_offline()\n",
    "        return agent\n",
    "    \n",
    "\n",
    "    def evaluate(self,agent,env,n_episodes=10,seed=1,visualize=False,eval_espilon=0.001):\n",
    "        epsiode_rewards = []\n",
    "        for episode in range(1,n_episodes+1):\n",
    "            state,info = env.reset()\n",
    "            #state = torch.tensor(state,dtype=torch.float32,device=agent.device)\n",
    "            #state = torch.unsqueeze(state,0)\n",
    "            #action = agent.greedy_policy(state,eval_espilon)\n",
    "            action = agent.epsilon_greedy_policy(state)\n",
    "            done = False\n",
    "            epsiode_reward = 0\n",
    "            episode_len = 0\n",
    "            while not done:\n",
    "                state,reward,terminated,truncated,info=env.step(action)\n",
    "                #state = torch.tensor(state,dtype=torch.float32,device=agent.device)\n",
    "                #state = torch.unsqueeze(state,0)\n",
    "                epsiode_reward += reward\n",
    "                done = terminated or truncated\n",
    "                #action = agent.greedy_policy(state,eval_espilon)\n",
    "                action = agent.epsilon_greedy_policy(state)\n",
    "                episode_len += 1\n",
    "            epsiode_rewards.append(epsiode_reward)\n",
    "            self.episode_summarize(episode,epsiode_reward)\n",
    "            #if(visualize):\n",
    "            #    env.summarize()\n",
    "        mean_rew = np.average(epsiode_rewards)\n",
    "        std_rew = np.std(epsiode_rewards)\n",
    "        return (mean_rew,std_rew) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "9737b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 486.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 22.0\n",
      "Episode: 2, Reward: 14.0\n",
      "Episode: 3, Reward: 22.0\n",
      "Episode: 4, Reward: 13.0\n",
      "Episode: 5, Reward: 13.0\n",
      "Episode: 6, Reward: 17.0\n",
      "Episode: 7, Reward: 28.0\n",
      "Episode: 8, Reward: 19.0\n",
      "Episode: 9, Reward: 30.0\n",
      "Episode: 10, Reward: 15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2057/10000 [00:04<00:21, 363.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:2000, Episode:76 Mean_Epi_Len: 28.68,Mean_Epi_Rew 29.68, Loss:  0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4029/10000 [00:10<00:23, 253.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:4000, Episode:119 Mean_Epi_Len: 50.53,Mean_Epi_Rew 51.53, Loss:  0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6036/10000 [00:17<00:15, 257.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:6000, Episode:139 Mean_Epi_Len: 92.47,Mean_Epi_Rew 93.47, Loss:  1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8065/10000 [00:25<00:06, 290.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:8000, Episode:151 Mean_Epi_Len: 139.11,Mean_Epi_Rew 140.11, Loss:  2.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:31<00:00, 320.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:10000, Episode:161 Mean_Epi_Len: 180.47,Mean_Epi_Rew 181.47, Loss:  2.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\") # just create gym environment natively\n",
    "torch.set_num_threads(1)  \n",
    "n_state = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent_parameters = { # this is where you can choose neural networks adjust hyperparameters \n",
    "'network_config': {\n",
    "    'state_dim': n_state,\n",
    "    'num_hidden_units': 128,\n",
    "    'num_actions': n_actions,\n",
    "    \"network_type\":\"dqn\"\n",
    "},\n",
    "'replay_buffer_size': 1_000_000,\n",
    "'minibatch_sz': 32,\n",
    "'observation_size':n_state,\n",
    "'num_replay_updates_per_step': 1,\n",
    "\"step_size\": 3e-4,\n",
    "'gamma': 0.99,\n",
    "'epsilon': 1,\n",
    "'update_freq':100,\n",
    "'warmup_steps':1000,\n",
    "'double_dqn':False\n",
    "}\n",
    "buffer_parameters = {\n",
    "    \"replay_buffer_size\":1_000_000,\n",
    "    \"minibatch_sz\":32,\n",
    "    \"observation_size\":n_state\n",
    "}\n",
    "#agent = DQNAgent(buffer_parameters) # You can change to other agents such as SARSA, ActorCritic\n",
    "agent = offlineDQNAgent(buffer_parameters)\n",
    "agent.set_device(device=\"cpu\")\n",
    "rl = RL() # You need this to control the overall process of training\n",
    "s,_=env.reset(seed=1)\n",
    "agent.set_seed(1)\n",
    "agent.replay_buffer.set_seed(1)\n",
    "#trained_agent = rl.learn(agent,env,agent_parameters,NSTEPS=100_000,output_step=2000,visualize=False,save_best_weights=False) # training loop\n",
    "trained_agent = rl.learn_offline(agent,env,agent_parameters,NSTEPS=1_000)\n",
    "rl.evaluate(trained_agent,env)\n",
    "trained_agent2 = rl.learn(trained_agent,env,agent_parameters,NSTEPS=10_000,output_step=2000,visualize=False,save_best_weights=False) # training loop\n",
    "rl.evaluate(trained_agent2,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78303066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
